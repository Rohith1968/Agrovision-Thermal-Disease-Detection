{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cRttaKACFe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e71513-6ac8-4b19-8112-45b4cec05d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/dataset/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiJ__Am08YUx",
        "outputId": "9b92ee30-cc1d-46f6-f77b-f08ea88b1f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test  train  validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/dataset/train'\n",
        "val_dir = '/content/drive/MyDrive/dataset/validation'\n",
        "test_dir = '/content/drive/MyDrive/dataset/test'\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "OOLUpa588eeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Path to image directories\n",
        "train_dir = '/content/drive/MyDrive/dataset/train'  # Adjust the path\n",
        "img_size = (150, 150)  # Resize images to 150x150\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_images(directory, img_size):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for label in ['healthy', 'infected']:  # Ensure 'healthy' and 'infected' are folder names\n",
        "        label_dir = os.path.join(directory, label)\n",
        "        for img_filename in os.listdir(label_dir):\n",
        "            img_path = os.path.join(label_dir, img_filename)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
        "            img_resized = cv2.resize(img, img_size)  # Resize image\n",
        "            img_normalized = img_resized / 255.0  # Normalize pixel values\n",
        "            images.append(img_normalized)\n",
        "            labels.append(0 if label == 'healthy' else 1)  # 0 for healthy, 1 for infected\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load and preprocess the images\n",
        "images, labels = load_and_preprocess_images(train_dir, img_size)\n",
        "\n",
        "# Reshape images to add a channel dimension (for grayscale)\n",
        "images = np.expand_dims(images, axis=-1)\n",
        "\n",
        "# Split data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape of the data\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dbeum1m7LvT",
        "outputId": "c068f68b-353a-4750-da19-32738b287649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (80, 150, 150, 1)\n",
            "Validation data shape: (21, 150, 150, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Convert pixel values to temperatures\n",
        "min_temp = 20  # Minimum temperature (assumption)\n",
        "max_temp = 80  # Maximum temperature (assumption)\n",
        "\n",
        "def pixel_to_temperature(img):\n",
        "    # Normalize pixel values to temperature range\n",
        "    temp_img = img * (max_temp - min_temp) / 255.0 + min_temp\n",
        "    return temp_img\n"
      ],
      "metadata": {
        "id": "kIhxhciBAgMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Path to the image directory\n",
        "train_dir = '/content/drive/MyDrive/dataset/train'  # Update path as necessary\n",
        "img_size = (150, 150)  # Resize images to 150x150 pixels\n",
        "\n",
        "# Temperature normalization function\n",
        "def pixel_to_temperature(img, min_temp, max_temp):\n",
        "    return img * (max_temp - min_temp) / 255.0 + min_temp\n",
        "\n",
        "# Function to load and preprocess thermal images with temperature normalization\n",
        "def load_and_preprocess_images(directory, img_size, min_temp=20, max_temp=80):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for label in ['healthy', 'infected']:\n",
        "        label_dir = os.path.join(directory, label)\n",
        "        for img_filename in os.listdir(label_dir):\n",
        "            img_path = os.path.join(label_dir, img_filename)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
        "            img_resized = cv2.resize(img, img_size)  # Resize image to uniform size\n",
        "            img_temp_normalized = pixel_to_temperature(img_resized, min_temp, max_temp)  # Convert to temperature values\n",
        "            img_normalized = img_temp_normalized / max_temp  # Normalize temperature values to [0, 1]\n",
        "            images.append(img_normalized)\n",
        "            labels.append(0 if label == 'healthy' else 1)  # Assign labels (0 = healthy, 1 = infected)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load and preprocess images\n",
        "images, labels = load_and_preprocess_images(train_dir, img_size)\n",
        "\n",
        "# Reshape images to add channel dimension for CNN input\n",
        "images = np.expand_dims(images, axis=-1)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape of the data\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XQi7fOlAnRm",
        "outputId": "4f2adba2-8378-4f88-be16-9ce9968a589e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (80, 150, 150, 1)\n",
            "Validation data shape: (21, 150, 150, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply edge detection\n",
        "def apply_edge_detection(img):\n",
        "    edges = cv2.Canny(img.astype(np.uint8), 100, 200)  # Apply Canny edge detection\n",
        "    return edges\n",
        "\n",
        "# Applying edge detection to preprocessed images\n",
        "def load_and_preprocess_images_with_edges(directory, img_size, min_temp=20, max_temp=80):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for label in ['healthy', 'infected']:\n",
        "        label_dir = os.path.join(directory, label)\n",
        "        for img_filename in os.listdir(label_dir):\n",
        "            img_path = os.path.join(label_dir, img_filename)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img_resized = cv2.resize(img, img_size)\n",
        "            img_temp_normalized = pixel_to_temperature(img_resized, min_temp, max_temp)\n",
        "            img_normalized = img_temp_normalized / max_temp\n",
        "            img_edges = apply_edge_detection(img_normalized)  # Apply edge detection\n",
        "            images.append(img_edges)\n",
        "            labels.append(0 if label == 'healthy' else 1)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Use this updated function for loading data if you want edge detection\n"
      ],
      "metadata": {
        "id": "SIIlqSRtAzxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Create a CNN model\n",
        "def create_cnn_model(input_shape):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Convolutional layers\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Flatten the output and feed into dense layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "    # Output layer for binary classification\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Get input shape from training data\n",
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "# Create the model\n",
        "model = create_cnn_model(input_shape)\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "pbXOhXF_A6P8",
        "outputId": "7392f8d8-aa93-4cfe-8ab1-d34c01a377f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m320\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m4,735,104\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,735,104</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,827,905\u001b[0m (18.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,827,905</span> (18.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,827,905\u001b[0m (18.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,827,905</span> (18.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image augmentation (optional)\n",
        "train_datagen = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                                   width_shift_range=0.2, height_shift_range=0.2,\n",
        "                                   shear_range=0.15, horizontal_flip=True)\n",
        "\n",
        "val_datagen = ImageDataGenerator()  # No augmentation for validation\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=val_datagen.flow(X_val, y_val),\n",
        "    epochs=50\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bxVEUIgBHoK",
        "outputId": "f7917c65-31b3-4bb4-bbcd-a2d78f78fe5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5023 - loss: 0.7095 - val_accuracy: 0.5714 - val_loss: 0.6920\n",
            "Epoch 2/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.4677 - loss: 0.6937 - val_accuracy: 0.4286 - val_loss: 0.6941\n",
            "Epoch 3/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5227 - loss: 0.6927 - val_accuracy: 0.4286 - val_loss: 0.6970\n",
            "Epoch 4/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5344 - loss: 0.6913 - val_accuracy: 0.4286 - val_loss: 0.7114\n",
            "Epoch 5/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5240 - loss: 0.6932 - val_accuracy: 0.4286 - val_loss: 0.7108\n",
            "Epoch 6/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.5031 - loss: 0.6981 - val_accuracy: 0.4286 - val_loss: 0.7013\n",
            "Epoch 7/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5188 - loss: 0.6927 - val_accuracy: 0.4286 - val_loss: 0.7012\n",
            "Epoch 8/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.5292 - loss: 0.6920 - val_accuracy: 0.4286 - val_loss: 0.7016\n",
            "Epoch 9/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.4953 - loss: 0.6953 - val_accuracy: 0.4286 - val_loss: 0.7012\n",
            "Epoch 10/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5135 - loss: 0.6930 - val_accuracy: 0.4286 - val_loss: 0.7028\n",
            "Epoch 11/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.5917 - loss: 0.6851 - val_accuracy: 0.4286 - val_loss: 0.7088\n",
            "Epoch 12/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5500 - loss: 0.6885 - val_accuracy: 0.4286 - val_loss: 0.7122\n",
            "Epoch 13/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5396 - loss: 0.6908 - val_accuracy: 0.4286 - val_loss: 0.7163\n",
            "Epoch 14/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.5552 - loss: 0.6878 - val_accuracy: 0.4286 - val_loss: 0.7196\n",
            "Epoch 15/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.5214 - loss: 0.6962 - val_accuracy: 0.4286 - val_loss: 0.7111\n",
            "Epoch 16/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 992ms/step - accuracy: 0.5539 - loss: 0.6871 - val_accuracy: 0.4286 - val_loss: 0.7064\n",
            "Epoch 17/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.4823 - loss: 0.6993 - val_accuracy: 0.4286 - val_loss: 0.7005\n",
            "Epoch 18/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.5344 - loss: 0.6912 - val_accuracy: 0.4286 - val_loss: 0.6990\n",
            "Epoch 19/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5461 - loss: 0.6905 - val_accuracy: 0.4286 - val_loss: 0.6988\n",
            "Epoch 20/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.5656 - loss: 0.6892 - val_accuracy: 0.4286 - val_loss: 0.7000\n",
            "Epoch 21/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5083 - loss: 0.6933 - val_accuracy: 0.4286 - val_loss: 0.7005\n",
            "Epoch 22/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.5292 - loss: 0.6915 - val_accuracy: 0.4286 - val_loss: 0.7019\n",
            "Epoch 23/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 995ms/step - accuracy: 0.4719 - loss: 0.6975 - val_accuracy: 0.4286 - val_loss: 0.7027\n",
            "Epoch 24/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.4849 - loss: 0.6963 - val_accuracy: 0.4286 - val_loss: 0.7043\n",
            "Epoch 25/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5344 - loss: 0.6908 - val_accuracy: 0.4286 - val_loss: 0.7074\n",
            "Epoch 26/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5188 - loss: 0.6933 - val_accuracy: 0.4286 - val_loss: 0.7081\n",
            "Epoch 27/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5344 - loss: 0.6909 - val_accuracy: 0.4286 - val_loss: 0.7085\n",
            "Epoch 28/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5422 - loss: 0.6900 - val_accuracy: 0.4286 - val_loss: 0.7065\n",
            "Epoch 29/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5266 - loss: 0.6921 - val_accuracy: 0.4286 - val_loss: 0.7033\n",
            "Epoch 30/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 994ms/step - accuracy: 0.5344 - loss: 0.6909 - val_accuracy: 0.4286 - val_loss: 0.7010\n",
            "Epoch 31/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.5578 - loss: 0.6888 - val_accuracy: 0.4286 - val_loss: 0.7000\n",
            "Epoch 32/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.5474 - loss: 0.6901 - val_accuracy: 0.4286 - val_loss: 0.7003\n",
            "Epoch 33/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.5227 - loss: 0.6922 - val_accuracy: 0.4286 - val_loss: 0.7008\n",
            "Epoch 34/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5227 - loss: 0.6921 - val_accuracy: 0.4286 - val_loss: 0.7014\n",
            "Epoch 35/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5383 - loss: 0.6906 - val_accuracy: 0.4286 - val_loss: 0.7025\n",
            "Epoch 36/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5370 - loss: 0.6906 - val_accuracy: 0.4286 - val_loss: 0.7044\n",
            "Epoch 37/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5344 - loss: 0.6909 - val_accuracy: 0.4286 - val_loss: 0.7063\n",
            "Epoch 38/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.5604 - loss: 0.6872 - val_accuracy: 0.4286 - val_loss: 0.7094\n",
            "Epoch 39/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5292 - loss: 0.6919 - val_accuracy: 0.4286 - val_loss: 0.7110\n",
            "Epoch 40/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5396 - loss: 0.6909 - val_accuracy: 0.4286 - val_loss: 0.7124\n",
            "Epoch 41/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.5695 - loss: 0.6856 - val_accuracy: 0.4286 - val_loss: 0.7142\n",
            "Epoch 42/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.5865 - loss: 0.6817 - val_accuracy: 0.4286 - val_loss: 0.7181\n",
            "Epoch 43/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 995ms/step - accuracy: 0.5070 - loss: 0.6995 - val_accuracy: 0.4286 - val_loss: 0.7151\n",
            "Epoch 44/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5188 - loss: 0.6951 - val_accuracy: 0.4286 - val_loss: 0.7108\n",
            "Epoch 45/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.5500 - loss: 0.6881 - val_accuracy: 0.4286 - val_loss: 0.7088\n",
            "Epoch 46/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.5031 - loss: 0.6968 - val_accuracy: 0.4286 - val_loss: 0.7061\n",
            "Epoch 47/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5813 - loss: 0.6842 - val_accuracy: 0.4286 - val_loss: 0.7058\n",
            "Epoch 48/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5266 - loss: 0.6920 - val_accuracy: 0.4286 - val_loss: 0.7042\n",
            "Epoch 49/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5305 - loss: 0.6913 - val_accuracy: 0.4286 - val_loss: 0.7036\n",
            "Epoch 50/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.5214 - loss: 0.6925 - val_accuracy: 0.4286 - val_loss: 0.7029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Save the model\n",
        "model.save('thermal_image_classifier.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqMHqmqdCGzm",
        "outputId": "4a4eedfe-16a9-4e05-a070-ce360e596ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.4286 - loss: 0.7029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.7029162645339966\n",
            "Validation Accuracy: 0.4285714328289032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_img_path = '/path/to/test/image.jpg'  # Replace with your actual test image path\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(test_img_path):\n",
        "    print(f\"Image found: {test_img_path}\")\n",
        "else:\n",
        "    print(f\"Image not found: {test_img_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpCRVFyqCYLI",
        "outputId": "90a437f4-c992-4382-ec1d-d9b94b13de82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image not found: /path/to/test/image.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_path = '/content/drive/MyDrive/dataset/test/your_test_image.jpg'\n"
      ],
      "metadata": {
        "id": "qZkeyKvdDAh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Directory where your images are stored\n",
        "image_dir = '/content/drive/MyDrive/dataset/'\n",
        "\n",
        "# Initialize the data generator (no separate classes, just loading)\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Use 20% of data for validation\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    image_dir,\n",
        "    target_size=(150, 150),  # You can adjust the size based on your image resolution\n",
        "    batch_size=32,\n",
        "    class_mode=None,  # No classes yet, since we don't have labels\n",
        "    shuffle=True,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    image_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode=None,  # No classes yet\n",
        "    shuffle=True,\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y1zUKWW9Anu",
        "outputId": "ec2f5fd4-ee27-4f71-c968-4aec59c3a8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202 images belonging to 3 classes.\n",
            "Found 49 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def load_images_from_directory(directory, target_size=(150, 150)):\n",
        "    images = []\n",
        "    image_filenames = [f for f in os.listdir(directory) if f.endswith(('jpg', 'jpeg', 'png'))]\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(directory, filename)\n",
        "        img = load_img(img_path, target_size=target_size)\n",
        "        img_array = img_to_array(img)\n",
        "        images.append(img_array)\n",
        "    return np.array(images)\n",
        "\n",
        "# Load images from the directory\n",
        "image_dir = '/content/drive/MyDrive/dataset/'  # Adjust to your path\n",
        "train_images = load_images_from_directory(image_dir)\n",
        "\n",
        "# Normalize the images (as we were doing with rescale=1./255)\n",
        "train_images = train_images / 255.0\n",
        "\n",
        "# Split into training and validation sets (80% train, 20% validation)\n",
        "split_idx = int(0.8 * len(train_images))\n",
        "train_data = train_images[:split_idx]\n",
        "val_data = train_images[split_idx:]\n"
      ],
      "metadata": {
        "id": "6pU5Aek79CxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the autoencoder with the loaded images\n",
        "autoencoder.fit(train_data, train_data,  # Input and output are the same\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                validation_data=(val_data, val_data))\n"
      ],
      "metadata": {
        "id": "ZV_uGRhe9fzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the train and validation data\n",
        "print(f\"Train data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n"
      ],
      "metadata": {
        "id": "Y2qQOWdQ9pVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "def load_images_from_directory(directory, target_size=(150, 150)):\n",
        "    images = []\n",
        "    image_filenames = [f for f in os.listdir(directory) if f.endswith(('jpg', 'jpeg', 'png'))]\n",
        "    print(f\"Found {len(image_filenames)} images in {directory}\")\n",
        "\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(directory, filename)\n",
        "        print(f\"Loading image: {img_path}\")\n",
        "\n",
        "        try:\n",
        "            img = load_img(img_path, target_size=target_size)  # Resize images\n",
        "            img_array = img_to_array(img)  # Convert to array (150, 150, 3)\n",
        "            images.append(img_array)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    return np.array(images)\n",
        "\n",
        "# Load images from the directory\n",
        "train_images = load_images_from_directory(image_dir)\n",
        "\n",
        "# Check if any images were loaded\n",
        "print(f\"Loaded {len(train_images)} images.\")\n",
        "\n",
        "# Normalize the images (rescale pixel values)\n",
        "if len(train_images) > 0:\n",
        "    train_images = train_images / 255.0\n",
        "\n",
        "    # Split into training and validation sets (80% train, 20% validation)\n",
        "    split_idx = int(0.8 * len(train_images))\n",
        "    train_data = train_images[:split_idx]\n",
        "    val_data = train_images[split_idx:]\n",
        "\n",
        "    print(f\"Train data shape: {train_data.shape}\")\n",
        "    print(f\"Validation data shape: {val_data.shape}\")\n",
        "else:\n",
        "    print(\"No images were loaded.\")\n"
      ],
      "metadata": {
        "id": "uLKJ4wok92Ol"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}